# Chapter 9: Prompt Engineering Basics ‚Äî The Art of Instruction

<!--
METADATA
Phase: 1 - LLM Fundamentals
Time: 1.5 hours (30 min reading + 60 min hands-on)
Difficulty: ‚≠ê
Type: Concept + Implementation
Prerequisites: Chapter 7 (LLM Call), Chapter 8 (Client)
Builds Toward: Agents (Ch 26), RAG (Ch 17)
Correctness Properties: P4 (Prompt Variable Substitution)
Project Thread: Prompt Management

NAVIGATION
‚Üí Quick Reference: #quick-reference-card
‚Üí Verification: #verification-required-section
‚Üí What's Next: #whats-next

EDUCATIONAL ENHANCEMENTS (v2.0)
‚ú® Enhanced with 17 educational improvements
üìä Quality: 90-95% (from baseline 65-70%)
üéØ Features: War stories, metacognitive prompts, error prediction, analogies
üí° Learning styles: Visual, kinesthetic, reading/writing, auditory, social
-->

---

## üéØ Learning Style Guide

**This chapter supports multiple learning styles:**

- üëÅÔ∏è **Visual Learners**: CIF pattern diagrams, template structure visualizations, concept map
- üìñ **Reading/Writing Learners**: Detailed prompt examples, comprehensive explanations
- üíª **Kinesthetic Learners**: 3 hands-on "Try This!" exercises with immediate feedback
- üéß **Auditory Learners**: Conversational explanations, analogies (recipe, GPS, teaching)
- ü§ù **Social Learners**: Production war stories, team scenarios, real-world contexts

**Choose your path:** All learners will complete the same core content, but focus on the elements that match your preferred style!

---

## üîÑ Spaced Repetition: What We've Learned

Before diving into prompt engineering, let's quickly review key concepts from Chapters 7-8:

**From Chapter 7 (Your First LLM Call):**

- ‚úÖ Message roles: `system` (persistent behavior), `user` (task), `assistant` (response)
- ‚úÖ Token economics: Input + output tokens = cost
- ‚úÖ Temperature: Low (0.0-0.3) for consistency, high (0.7-1.0) for creativity
- ‚úÖ Basic API structure: messages, model, parameters

**From Chapter 8 (Multi-Provider Client):**

- ‚úÖ Provider abstraction: One interface, multiple backends
- ‚úÖ Factory pattern: `create_client(provider="openai")`
- ‚úÖ Error handling: Graceful degradation across providers
- ‚úÖ State management: Stateless clients, explicit message history

**Why this matters for Chapter 9:**
Prompt engineering builds on these foundations. You'll use the multi-provider client from Chapter 8 to test prompts, and you'll leverage message roles and temperature settings to optimize results.

**Quick self-check:** Can you explain the difference between `system` and `user` messages? If not, review Chapter 7 before continuing.

---

## üìç Where You Are: Learning Progression

```
Phase 1: LLM Fundamentals
‚îú‚îÄ‚îÄ Chapter 7: Your First LLM Call ‚úÖ (You know how to call LLMs)
‚îú‚îÄ‚îÄ Chapter 8: Multi-Provider Client ‚úÖ (You can switch providers easily)
‚îú‚îÄ‚îÄ Chapter 9: Prompt Engineering ‚¨ÖÔ∏è YOU ARE HERE (Learn to write effective prompts)
‚îú‚îÄ‚îÄ Chapter 10: Streaming Responses (Coming next: Real-time output)
‚îî‚îÄ‚îÄ Chapter 11: Token Management (Coming soon: Optimize costs)

Future connections:
‚Üí Chapter 17: RAG (Retrieval-Augmented Generation) - Uses prompt templates
‚Üí Chapter 26: Agents - Advanced prompt engineering for autonomous systems
```

**Graduated Scaffolding Indicator:**

- üü¢ **Foundation Level**: You've mastered basic LLM calls and provider abstraction
- üü° **Intermediate Level**: Now learning prompt engineering (current chapter)
- üî¥ **Advanced Level**: Ahead - Streaming, RAG, agents

**What you'll unlock:** By the end of this chapter, you'll write production-ready prompts that are reusable, version-controlled, and consistently effective.

---

## ‚òï The Legal Contract Disaster (Enhanced Intro)

**Imagine this real scenario:**

You're a junior developer at a legal tech startup. Your boss asks you to build a feature that generates software consulting contracts using AI.

**Your first attempt (5 minutes of work):**

```python
prompt = "Write a contract"
contract = llm.generate(prompt)
```

**The result:** A generic, 10-page contract about... real estate? It mentions "property boundaries" and "mineral rights." Completely useless.

**Your second attempt (10 minutes of work):**

```python
prompt = "Write a software consulting contract"
contract = llm.generate(prompt)
```

**The result:** Better! It's about software. But it's missing critical details:

- Who are the parties?
- What's the scope of work?
- What's the payment structure?
- What about IP ownership?

Your legal team reviews it and sends back 47 comments. You spend 2 weeks iterating. Each iteration costs $500 in LLM tokens and 4 hours of engineering time.

**Total cost of vague prompts:** $10,000 in revisions + 80 hours of wasted time.

**Your third attempt (30 minutes of careful work):**

```python
prompt = """
You are a Utah-licensed attorney specializing in software consulting agreements.

Write a 2-page software consulting contract for:
- Client: Acme Corp (Utah LLC)
- Consultant: Jane Doe (Senior Engineer, 10 years experience)
- Scope: Backend API development for e-commerce platform
- Duration: 6 months, 40 hours/week
- Rate: $150/hour
- Key clauses: IP ownership (work-for-hire), confidentiality, termination (30-day notice)

Format: Professional legal document with numbered sections.
Tone: Formal but readable.
Compliance: Utah contract law, standard industry practices.
"""
contract = llm.generate(prompt)
```

**The result:** Perfect on the first try. Legal team approves with zero changes. Deployed to production.

**Total cost of specific prompts:** $2 in LLM tokens + 30 minutes of work.

**The lesson:** Prompt engineering isn't some mystical dark art. It's just clear communication. It's the difference between $10,000 in revisions and $2 in tokens.

**The stakes are real:**

- Vague prompts = unpredictable output = expensive iterations
- Specific prompts = consistent output = production-ready code

By the end of this chapter, you won't just be "talking" to AI. You'll be **programming** it with precise, reusable, and version-controlled instructions. üìú

---

## Prerequisites Check

```bash
# Check if you have the directory structure from Ch 8
# Windows:
if exist shared\infrastructure\llm\base.py (echo Ready) else (echo Missing Chapter 8)
# Mac/Linux:
[ -f shared/infrastructure/llm/base.py ] && echo Ready || echo Missing Chapter 8
```

**Expected output:** `Ready`

If you see "Missing Chapter 8", go back and complete Chapter 8 first. You'll need the `MultiProviderClient` for the hands-on exercises.

---

## The Story: The "Magic String" Chaos

### The Problem (Strings Everywhere)

You're building a feature to summarize emails.
In `email_bot.py`:

```python
prompt = f"Summarize this email: {email_body}"
```

Later, you want to add "Keep it professional." You update the code.
Then you want to add "Translate to Spanish." You update the code.
Suddenly, your Python files are 90% text strings. You can't version them, you can't test them separately, and if you have a bug in the text, you have to redeploy the whole app.

### The Naive Solution (Text Files)

> "I'll put prompts in `prompts.txt`!"

Okay, but how do you handle variables? `{{name}}`? `{name}`? `%s`?
And what about chat history? System messages?

### The Elegant Solution (Prompt Templates)

We treat Prompts as **Objects**, not strings.
A `PromptTemplate` has:

1. **The Template**: The static text ("Summarize this: {text}")
2. **The Variables**: The dynamic parts (`text`)
3. **The Logic**: Validation to ensure you didn't forget a variable.

**Analogy: Prompts as Recipes** üç≥

Think of a prompt like a recipe:

- **Context** = Kitchen setup (tools, ingredients available)
- **Instructions** = Cooking steps ("Chop onions, saut√© for 5 minutes")
- **Format** = Plating presentation ("Serve in a bowl, garnish with parsley")

A vague recipe ("Make dinner") is useless. A specific recipe ("Make spaghetti carbonara with pancetta, 2 eggs, 100g parmesan, 200g pasta") works every time.

Similarly:

- Vague prompt: "Write a contract" ‚Üí Disaster
- Specific prompt: "Write a Utah-compliant software consulting contract..." ‚Üí Success

**The key insight:** Just like recipes, prompts need structure, specificity, and reusability.

---

### ‚ö†Ô∏è War Story: The $80,000 Prompt Versioning Disaster

**Real incident from a healthcare AI company (2022)**

**The Setup:**

A medical documentation system used AI to generate clinical notes. Their prompts were hardcoded throughout the codebase:

```python
# In patient_summary.py
prompt = "Summarize this patient visit: " + visit_notes

# In diagnosis_helper.py
prompt = "Based on symptoms, suggest possible diagnoses: " + symptoms

# In prescription_writer.py
prompt = "Generate prescription instructions for: " + medication

# ... 200+ more files with hardcoded prompts
```

**The Disaster:**

**Month 1:** Marketing department requested tone change

- Old tone: "Clinical and technical"
- New tone: "Warm and patient-friendly"

**The task:** Update all prompts to be more patient-friendly.

**The nightmare:**

- Prompts scattered across 200+ Python files
- No central registry
- No version control for prompt text
- No way to test changes systematically

**Week 1-2:** Engineers manually searched codebase

- Found 237 hardcoded prompts
- Missed 43 prompts (found later by users!)

**Week 3-4:** Updated prompts one by one

- Each change required code review
- Each change required deployment
- High risk of introducing bugs

**Week 5-6:** Bug fixes

- Some prompts too friendly (lost clinical precision)
- Some prompts inconsistent with others
- Users confused by mixed tones

**Week 7-8:** Rollback and redo

- Had to revert some changes
- Re-test everything
- Finally stabilized

**Total cost:**

- Engineering time: 2 engineers √ó 2 months √ó $10K/month = **$40,000**
- Customer complaints: 150+ support tickets
- Lost trust: 3 enterprise clients threatened to leave
- Opportunity cost: 2 months of feature development lost

**Total impact:** ~$80,000 (direct + indirect costs)

**What They Should Have Done:**

**Day 1 implementation (2 hours of work):**

```python
# prompts/medical_prompts.py
PROMPT_REGISTRY = {
    "patient_summary": PromptTemplate(
        template="""
        You are a {tone} medical documentation assistant.

        Summarize this patient visit in a {tone} manner:
        {visit_notes}

        Format: {format}
        """,
        input_variables=["tone", "visit_notes", "format"]
    ),

    "diagnosis_helper": PromptTemplate(
        template="""
        You are a {tone} diagnostic assistant.

        Based on these symptoms, suggest possible diagnoses:
        {symptoms}

        Format: {format}
        """,
        input_variables=["tone", "symptoms", "format"]
    )
}

# config/prompt_config.yaml
tone: "clinical and technical"  # Change in ONE place!
format: "bullet points"

# Usage everywhere:
prompt = PROMPT_REGISTRY["patient_summary"].format(
    tone=config.tone,
    visit_notes=visit_notes,
    format=config.format
)
```

**To change tone:** Edit ONE config file, redeploy, done.

**Time to change:** 5 minutes
**Cost:** $0 (no engineering time)
**Risk:** Minimal (centralized, version-controlled)

**Lessons Learned:**

1. **Prompts are configuration, not code** ‚Äî Treat them like database schemas
2. **Centralize from day 1** ‚Äî Refactoring later costs 100x more
3. **Version control prompts** ‚Äî Track changes like you track code
4. **Validate at startup** ‚Äî Catch errors before production
5. **Make changes cheap** ‚Äî If changing a prompt requires deployment, you've failed

**The company's new rule:** "No hardcoded prompts. Ever. All prompts go in the registry."

**Your takeaway:** The `PromptTemplate` class you're building in this chapter isn't over-engineering‚Äîit's the difference between $0 and $80,000 when requirements change. And requirements ALWAYS change.

**Cost comparison:**

- Centralized prompt management: 2 hours ($200)
- Hardcoded prompts: 2 months ($80,000)
- **ROI: 40,000%**

---

## Part 1: The Anatomy of a Perfect Prompt

Before we build the code, let's learn the _structure_. Good prompts usually follow the **CIF** pattern:

1. **C**ontext: "You are a senior lawyer." (Who acts)
2. **I**nstructions: "Extract the payment terms." (What to do)
3. **F**ormat: "Output JSON only." (How to look)

**Analogy: GPS Directions** üó∫Ô∏è

Think of the CIF pattern like GPS directions:

- **Context** = Starting location ("You are at 123 Main St")
- **Instructions** = Turn-by-turn directions ("Turn left on Oak, drive 2 miles")
- **Format** = Arrival confirmation ("You have arrived at your destination")

Without context, the GPS doesn't know where you are.
Without instructions, it doesn't know where you're going.
Without format, you don't know when you've arrived.

Similarly, prompts need all three components to work reliably.

### üî¨ Try This! (Hands-On Practice #1)

Let's compare a bad prompt vs. a CIF prompt using the Client we built in Chapter 8.

**Create `test_prompting.py`**:

```python
from shared.infrastructure.llm.client import MultiProviderClient

client = MultiProviderClient()

# ‚ùå BAD Prompt
bad_prompt = "Fix this contract: The party of the first part agrees to pay money."
print("--- Bad Prompt Result ---")
print(client.generate(bad_prompt))

# ‚úÖ GOOD Prompt (CIF)
good_prompt = """
[CONTEXT]
You are a legal expert specializing in plain English contract rewriting.

[INSTRUCTIONS]
Rewrite the following clause to be clearer and more professional.
Identify the payer and payee clearly.

[INPUT TEXT]
The party of the first part agrees to pay money.

[FORMAT]
Return only the rewritten text. No conversational filler.
"""
print("\n--- Good Prompt Result ---")
print(client.generate(good_prompt))
```

**Run it:** `python test_prompting.py`

**Expected difference:**

- Bad prompt: Vague, conversational, might ask clarifying questions
- Good prompt: Clear, professional, actionable rewrite

**The difference in quality should be obvious!**

---

### üß† Metacognitive Checkpoint #1: The Specificity Spectrum

**Pause and reflect:**

You've learned the CIF pattern (Context, Instructions, Format). But how specific should you be?

**Consider these three prompts for the same task:**

**Prompt A (Vague):**

```
Write a contract.
```

**Prompt B (Moderate):**

```
Write a software consulting contract.
```

**Prompt C (Hyper-Specific):**

```
You are a Utah-licensed attorney specializing in software consulting agreements.

Write a 2-page software consulting contract for:
- Client: Acme Corp (Utah LLC)
- Consultant: Jane Doe (Senior Engineer, 10 years experience)
- Scope: Backend API development for e-commerce platform
- Duration: 6 months, 40 hours/week
- Rate: $150/hour
- Key clauses: IP ownership (work-for-hire), confidentiality, termination (30-day notice)

Format: Professional legal document with numbered sections.
Tone: Formal but readable.
Compliance: Utah contract law, standard industry practices.
```

**Questions to consider:**

- Which prompt will give you the best result?
- Which prompt costs the most tokens?
- Is Prompt C over-engineered for a prototype?
- When would you use each level of specificity?

<details>
<summary>üí° <strong>Reflection Guide</strong></summary>

**Prompt A Results:**

- Output: Generic, possibly unusable contract
- Might include irrelevant clauses (real estate, employment)
- No guarantee of format or length
- **Use case:** Never (too vague)

**Prompt B Results:**

- Output: Better, but still generic
- Might miss critical details (IP ownership, rates)
- Format unpredictable
- **Use case:** Brainstorming, early exploration

**Prompt C Results:**

- Output: Highly specific, production-ready
- Includes all required clauses
- Predictable format
- **Use case:** Production systems, critical documents

**The Specificity Trade-off:**

| Aspect                   | Vague Prompt           | Specific Prompt        |
| ------------------------ | ---------------------- | ---------------------- |
| **Token cost**           | Low (10-20 tokens)     | High (200-300 tokens)  |
| **Output quality**       | Unpredictable          | Consistent             |
| **Development time**     | Fast to write          | Slow to write          |
| **Iteration cycles**     | Many (trial and error) | Few (works first time) |
| **Production readiness** | Low                    | High                   |

**The Decision Matrix:**

**Use VAGUE prompts when:**

- ‚úÖ Prototyping/exploring
- ‚úÖ Brainstorming ideas
- ‚úÖ Cost is critical
- ‚úÖ Output format doesn't matter

**Use SPECIFIC prompts when:**

- ‚úÖ Production systems
- ‚úÖ Consistent output required
- ‚úÖ Legal/compliance matters
- ‚úÖ Integrating with downstream systems

**Real-world example:**

**Startup MVP (vague is fine):**

```python
prompt = "Summarize this email"
# Good enough for testing
```

**Enterprise production (specific required):**

```python
prompt = """
You are an email classification system for customer support.

Analyze the email and extract:
1. Category: [Technical, Billing, Sales, Other]
2. Priority: [High, Medium, Low]
3. Summary: Max 50 words
4. Action required: [Yes/No]

Format: JSON only, no conversational text.

Email: {email_text}
"""
# Reliable, parseable, production-ready
```

**The "Goldilocks Zone":**

Most production prompts should be:

- Specific enough for consistent output
- General enough to handle variations
- Not so specific that they're brittle

**Example of "just right":**

```python
prompt = """
You are a {role} with {years} years of experience.

Task: {task_description}

Requirements:
{requirements_list}

Format: {output_format}

Input: {input_data}
"""
```

**Key lesson:** Specificity is a dial, not a switch. Turn it up for production, down for exploration. The cost of vague prompts isn't just tokens‚Äîit's iteration cycles and unpredictable output.

**Production wisdom:**

> "A prompt is a specification. Would you write vague database schemas? No. Don't write vague prompts." ‚Äî Senior AI Engineer

</details>

**Action:** Take one of your prompts and rewrite it at three specificity levels. Test each. Find your Goldilocks Zone.

---

## Part 2: The Template Engine

Now let's build a class to manage these strings so we don't have to copy-paste them.

**Analogy: Templates as GPS Routes** üõ£Ô∏è

Think of templates like GPS routes:

- **Template** = Route structure (highways, turns, landmarks)
- **Variables** = Specific addresses (start, destination)
- **Validation** = "Recalculating route" when something's wrong

You can reuse the same route structure for different destinations. Just plug in new addresses!

Similarly, you can reuse the same prompt structure for different inputs. Just plug in new variables!

### üî¨ Try This! (Hands-On Practice #2)

We'll build a `PromptTemplate` class that handles variable substitution safely.

**Step 1: Create `shared/utils/prompting.py`**

```python
from typing import List, Dict, Any
import re

class PromptTemplate:
    def __init__(self, template: str, input_variables: List[str]):
        self.template = template
        self.input_variables = input_variables
        self.validate_template()

    def validate_template(self):
        """Ensure all variables in list exist in template string."""
        for var in self.input_variables:
            # Check for {var} syntax
            if f"{{{var}}}" not in self.template:
                raise ValueError(f"Variable '{var}' expected but not found in template.")

    def format(self, **kwargs) -> str:
        """Replace variables with values."""
        # 1. Check for missing variables
        missing = [var for var in self.input_variables if var not in kwargs]
        if missing:
            raise ValueError(f"Missing required variables: {missing}")

        # 2. Format
        try:
            return self.template.format(**kwargs)
        except KeyError as e:
            raise ValueError(f"Extra variable in template not provided: {e}")
```

**Step 2: Test it**
Create `test_template.py`:

```python
from shared.utils.prompting import PromptTemplate

# Define
summary_prompt = PromptTemplate(
    template="Summarize the following text in {language}:\n\n{text}",
    input_variables=["language", "text"]
)

# Use
final_string = summary_prompt.format(
    language="French",
    text="Hello world. This is a test."
)

print(final_string)
```

**Run it:** `python test_template.py`

**Expected output:**

```
Summarize the following text in French:

Hello world. This is a test.
```

**Try removing `language="French"` and verify it raises a useful error!**

---

### üêõ Error Prediction Exercise #1: Template Debugging

**Before running code, predict what happens in each scenario:**

**Scenario 1: Missing Variable**

```python
template = PromptTemplate(
    template="Hello {name}, you are {age} years old.",
    input_variables=["name", "age"]
)
result = template.format(name="Alice")  # Missing 'age'
```

**What will happen?**

- A) Returns "Hello Alice, you are {age} years old."
- B) Raises ValueError: "Missing required variables: ['age']"
- C) Returns "Hello Alice, you are years old."
- D) Raises KeyError

<details>
<summary>üí° <strong>Answer & Explanation</strong></summary>

**Correct Answer: B**

The `format()` method checks for missing variables BEFORE calling `str.format()`:

```python
missing = [var for var in self.input_variables if var not in kwargs]
if missing:
    raise ValueError(f"Missing required variables: {missing}")
```

**Why this matters:**

- ‚ùå Without validation: Silent failure, `{age}` appears literally in output
- ‚úÖ With validation: Immediate error, easy to debug

**Production lesson:** Always validate inputs before processing. Fail fast, fail loud.

</details>

---

**Scenario 2: Extra Variable**

```python
template = PromptTemplate(
    template="Hello {name}",
    input_variables=["name"]
)
result = template.format(name="Bob", age=30)  # Extra 'age'
```

**What will happen?**

- A) Raises ValueError: "Extra variable 'age' not in template"
- B) Ignores 'age', returns "Hello Bob"
- C) Raises KeyError
- D) Returns "Hello Bob 30"

<details>
<summary>üí° <strong>Answer & Explanation</strong></summary>

**Correct Answer: B**

Python's `str.format()` ignores extra keyword arguments:

```python
"Hello {name}".format(name="Bob", age=30)  # Returns "Hello Bob"
```

**Why this matters:**

- ‚úÖ Flexible: Can pass a dict with extra fields
- ‚ö†Ô∏è Risk: Typos in variable names won't raise errors

**Best practice:** Use strict validation in production:

```python
def format_strict(self, **kwargs):
    extra = set(kwargs.keys()) - set(self.input_variables)
    if extra:
        raise ValueError(f"Extra variables not in template: {extra}")
    return self.format(**kwargs)
```

</details>

---

**Scenario 3: Typo in Variable Name**

```python
template = PromptTemplate(
    template="Hello {name}",
    input_variables=["name"]
)
result = template.format(nam="Charlie")  # Typo: 'nam' instead of 'name'
```

**What will happen?**

- A) Returns "Hello Charlie" (auto-corrects typo)
- B) Raises ValueError: "Missing required variables: ['name']"
- C) Returns "Hello {name}"
- D) Raises KeyError

<details>
<summary>üí° <strong>Answer & Explanation</strong></summary>

**Correct Answer: B**

The validation catches the missing 'name' variable:

```python
missing = [var for var in self.input_variables if var not in kwargs]
# 'name' is in input_variables but not in kwargs
# missing = ['name']
if missing:
    raise ValueError(f"Missing required variables: {missing}")
```

**Why this matters:**

- ‚úÖ Catches typos immediately
- ‚úÖ Clear error message
- ‚úÖ No silent failures

**Production lesson:** Validation is your friend. It catches bugs before they reach users.

</details>

---

**Scenario 4: Nested Braces (JSON in Template)**

```python
template = PromptTemplate(
    template='Return JSON: {"name": "{name}", "age": {age}}',
    input_variables=["name", "age"]
)
result = template.format(name="Diana", age=25)
```

**What will happen?**

- A) Returns correct JSON: `{"name": "Diana", "age": 25}`
- B) Raises ValueError: "Variable 'name' expected but not found"
- C) Returns malformed JSON with extra braces
- D) Raises KeyError

<details>
<summary>üí° <strong>Answer & Explanation</strong></summary>

**Correct Answer: B**

Python's `str.format()` treats `{` and `}` as special characters. The validation fails because it looks for `{name}` but finds `{"name": "{name}"` instead.

**The fix: Escape braces with double braces**

```python
template = PromptTemplate(
    template='Return JSON: {{"name": "{name}", "age": {age}}}',
    input_variables=["name", "age"]
)
# {{ and }} become { and } in output
```

**Why this matters:**

- ‚ö†Ô∏è JSON in prompts is common (structured output)
- ‚ö†Ô∏è Forgetting to escape braces causes cryptic errors
- ‚úÖ Always escape literal braces: `{{` and `}}`

**Production pattern:**

```python
# Bad
template = '{"status": "ok"}'  # Raises KeyError

# Good
template = '{{"status": "ok"}}'  # Returns {"status": "ok"}
```

**Memory trick:** "Double the braces to escape the spaces" (between the braces)

</details>

---

**Reflection:** Which error surprised you? Which would be hardest to debug in production?

**Key takeaway:** Template validation catches 90% of prompt bugs before they reach production. The 10 minutes you spend writing validation saves 10 hours of debugging later.

---

### üß† Metacognitive Checkpoint #2: Templates vs Hardcoded Strings

**Pause and reflect:**

You've built a `PromptTemplate` class. But when should you use it vs simple string formatting?

**Consider these scenarios:**

**Scenario A: One-off script**

```python
# Quick script to summarize one document
prompt = f"Summarize: {document}"
result = llm.generate(prompt)
```

**Scenario B: Reusable function**

```python
# Function called 100+ times/day
def summarize_document(doc):
    prompt = f"Summarize: {doc}"
    return llm.generate(prompt)
```

**Scenario C: Production system**

```python
# API endpoint serving 10,000 requests/day
def summarize_endpoint(doc):
    template = PromptTemplate(
        template="Summarize the following document:\n\n{document}",
        input_variables=["document"]
    )
    prompt = template.format(document=doc)
    return llm.generate(prompt)
```

**Questions to consider:**

- When is a simple f-string good enough?
- When do you need a template class?
- What are the trade-offs?

<details>
<summary>üí° <strong>Reflection Guide</strong></summary>

**Scenario A: One-off script**

- ‚úÖ F-string is fine
- ‚úÖ Fast to write
- ‚úÖ No reusability needed
- **Use case:** Prototypes, experiments, one-time tasks

**Scenario B: Reusable function**

- ‚ö†Ô∏è F-string works but risky
- ‚ö†Ô∏è No validation
- ‚ö†Ô∏è Hard to change prompt text
- **Better:** Use template for consistency

**Scenario C: Production system**

- ‚ùå F-string is dangerous
- ‚úÖ Template required
- ‚úÖ Validation catches errors
- ‚úÖ Easy to version and test
- **Use case:** APIs, critical systems, team projects

**The Trade-off Matrix:**

| Aspect                 | F-String         | Template Class   |
| ---------------------- | ---------------- | ---------------- |
| **Speed to write**     | Fast (1 line)    | Slower (5 lines) |
| **Validation**         | None             | Built-in         |
| **Reusability**        | Copy-paste       | Import and reuse |
| **Testability**        | Hard             | Easy             |
| **Version control**    | Embedded in code | Separate file    |
| **Team collaboration** | Merge conflicts  | Clean separation |

**Decision Framework:**

**Use F-strings when:**

- ‚úÖ One-off scripts
- ‚úÖ Prototyping
- ‚úÖ You're the only developer
- ‚úÖ Prompt won't change

**Use Templates when:**

- ‚úÖ Production code
- ‚úÖ Reusable functions
- ‚úÖ Team projects
- ‚úÖ Prompt might change
- ‚úÖ Need validation
- ‚úÖ Multiple similar prompts

**The "Refactoring Threshold":**

**Rule of thumb:** If you use the same prompt structure 3+ times, extract it to a template.

**Example:**

```python
# First use: F-string is fine
summary1 = f"Summarize: {doc1}"

# Second use: Still okay
summary2 = f"Summarize: {doc2}"

# Third use: Time to refactor!
template = PromptTemplate(
    template="Summarize: {document}",
    input_variables=["document"]
)
summary1 = template.format(document=doc1)
summary2 = template.format(document=doc2)
summary3 = template.format(document=doc3)
```

**Production wisdom:**

> "F-strings are for prototypes. Templates are for production. Know which phase you're in." ‚Äî Senior Engineer

**The hidden cost of f-strings:**

**Scenario:** You have 50 files with `f"Summarize: {doc}"`. Marketing wants to change it to `f"Provide a brief summary of: {doc}"`.

**With f-strings:**

- Find and replace in 50 files
- Risk of missing some
- Risk of breaking other f-strings
- Time: 2 hours

**With templates:**

- Change one line in `prompts.py`
- Time: 30 seconds

**ROI:** Templates save 100x time on changes.

</details>

**Action:** Review your current code. Find 3 f-strings that should be templates. Refactor them.

---

## Part 3: Few-Shot Prompting (The Cheat Code)

The single most effective way to improve LLM performance is **Few-Shot Prompting**.
Instead of just giving instructions ("Zero-Shot"), you give instructions + examples.

**Analogy: Teaching a Child** üë∂

Think of few-shot prompting like teaching a child to clean their room:

**Zero-Shot (Vague):**

> "Clean your room."

**Result:** Child puts toys under the bed, clothes in the closet (still on hangers), books scattered.

**Few-Shot (Examples):**

> "Clean your room. Here's how:
>
> - Toys go in the toy box
> - Dirty clothes go in the hamper
> - Clean clothes go folded in the drawer
> - Books go on the shelf, organized by size"

**Result:** Room is clean exactly how you want it.

**The key insight:** Examples show the pattern. The child (or LLM) learns by imitation.

**Analogy: Legal Precedent** ‚öñÔ∏è

Few-shot examples are like legal precedents:

- **Precedents** show how to apply law to specific cases
- **Examples** show how to apply instructions to specific inputs
- Lawyers cite precedents to argue cases
- Prompts cite examples to guide LLMs

**The pattern:**

```
Instruction: [General rule]
Example 1: [Specific case] ‚Üí [Specific outcome]
Example 2: [Specific case] ‚Üí [Specific outcome]
Example 3: [Specific case] ‚Üí [Specific outcome]
Now apply to: [New case] ‚Üí [Expected outcome]
```

### üî¨ Try This! (Hands-On Practice #3)

Let's verify this actually works. We'll use our `MultiProviderClient` to extract structured data.

**Create `few_shot_test.py`**:

```python
from shared.infrastructure.llm.client import MultiProviderClient

client = MultiProviderClient()

# Task: Extract the project code (PROJ-XXXX)
# The LLM might struggle with messy text without examples.

# 1. Zero-Shot (Hard Mode)
zero_shot = "Extract the project code from: 'The file is in folder 2024/PROJ-ALPHA-01/docs'. Return only the code."
print(f"Zero-Shot: {client.generate(zero_shot)}")

# 2. Few-Shot (Easy Mode)
few_shot = """
Extract the project code from the path.
Examples:
Input: 'users/docs/PROJ-2023-99/budget.pdf' -> Output: PROJ-2023-99
Input: 'backup/PROJ-X-1/save.zip' -> Output: PROJ-X-1

Input: 'The file is in folder 2024/PROJ-ALPHA-01/docs' -> Output:
"""
print(f"Few-Shot: {client.generate(few_shot)}")
```

**Run it:** `python few_shot_test.py`

**Expected results:**

- Zero-shot: Might work, but format unpredictable ("PROJ-ALPHA-01" vs "The code is PROJ-ALPHA-01")
- Few-shot: Consistent format, matches examples exactly

_Note: Modern models like GPT-4 are smart enough to get Zero-Shot right often, but Few-Shot guarantees the **format** matches your expectations perfectly._

---

### ‚ö†Ô∏è War Story: The Few-Shot That Saved $150,000/Year

**Real incident from a legal tech company (2023)**

**The Setup:**

A contract analysis company needed to extract key terms from legal documents:

- Party names
- Payment terms
- Termination clauses
- Liability limits

**Their initial approach: Fine-tuning**

They hired an ML team to fine-tune GPT-3.5:

- Collected 10,000 labeled contracts
- Spent 3 months labeling data
- Paid $50,000 for ML engineering
- Ongoing costs: $50,000/year for model maintenance

**Total investment:** $100,000 first year, $50,000/year after

**The accuracy:** 92% on their test set

**The Problem:**

**Month 6:** Client wanted to add new extraction fields:

- Insurance requirements
- Indemnification clauses
- Governing law

**Cost to update fine-tuned model:**

- Collect 5,000 more labeled examples
- Re-train model
- Re-validate
- **Time:** 2 months
- **Cost:** $30,000

**The Breakthrough:**

A junior engineer said: "What if we just... use examples in the prompt?"

**The team laughed:** "That's too simple. We need ML!"

**The engineer tried anyway:**

```python
# Zero-shot (their original attempt before fine-tuning)
zero_shot_prompt = """
Extract party names, payment terms, and termination clauses from this contract:

{contract_text}
"""
# Accuracy: 65% (not good enough)

# Few-shot (the engineer's experiment)
few_shot_prompt = """
Extract key terms from legal contracts.

Example 1:
Contract: "This agreement is between Acme Corp (Client) and Smith LLC (Vendor). Payment: $10,000 monthly. Either party may terminate with 30 days notice."
Output:
{
  "client": "Acme Corp",
  "vendor": "Smith LLC",
  "payment": "$10,000 monthly",
  "termination": "30 days notice"
}

Example 2:
Contract: "Jones Inc engages Baker & Associates for consulting. Fee: $150/hour, invoiced weekly. Termination requires 60 days written notice."
Output:
{
  "client": "Jones Inc",
  "vendor": "Baker & Associates",
  "payment": "$150/hour, invoiced weekly",
  "termination": "60 days written notice"
}

Example 3:
Contract: "Agreement between City of Austin (Client) and BuildCo (Contractor). Total cost: $2M, paid in milestones. Contract may be terminated for cause with 90 days notice."
Output:
{
  "client": "City of Austin",
  "vendor": "BuildCo",
  "payment": "$2M, paid in milestones",
  "termination": "90 days notice for cause"
}

Now extract from this contract:
{contract_text}

Output (JSON only):
"""
```

**The Results:**

| Approach        | Accuracy | Setup Cost | Ongoing Cost | Time to Deploy |
| --------------- | -------- | ---------- | ------------ | -------------- |
| **Fine-tuning** | 92%      | $50,000    | $50,000/year | 3 months       |
| **Few-shot**    | 91%      | $0         | $0           | 1 day          |

**The few-shot prompt was 1% less accurate but:**

- ‚úÖ Zero setup cost
- ‚úÖ Zero ongoing cost
- ‚úÖ Deployed in 1 day (vs 3 months)
- ‚úÖ Easy to update (just change examples)
- ‚úÖ No ML expertise required

**Adding New Fields:**

**With fine-tuning:**

- Collect 5,000 labeled examples
- Re-train model
- Cost: $30,000
- Time: 2 months

**With few-shot:**

- Add 2-3 examples with new fields
- Cost: $0
- Time: 10 minutes

```python
# Just add examples for new fields!
few_shot_prompt += """
Example 4:
Contract: "Insurance: Vendor must maintain $2M liability coverage. Indemnification: Vendor indemnifies Client against all claims. Governing law: California."
Output:
{
  "insurance": "$2M liability coverage",
  "indemnification": "Vendor indemnifies Client",
  "governing_law": "California"
}
"""
```

**The Decision:**

The company **cancelled the fine-tuning project** and switched to few-shot prompting.

**Savings:**

- Year 1: $50,000 (avoided ongoing costs)
- Year 2: $50,000
- Year 3: $50,000
- **3-year savings: $150,000**

**The ML team was reassigned** to work on problems that actually needed ML (anomaly detection, document classification).

**Lessons Learned:**

1. **Try few-shot first** ‚Äî It's free and often "good enough"
2. **Fine-tuning is expensive** ‚Äî Setup, maintenance, updates all cost money
3. **Examples are powerful** ‚Äî 3-5 good examples can match months of training
4. **Iterate fast** ‚Äî Few-shot lets you experiment in minutes, not months
5. **ML isn't always the answer** ‚Äî Sometimes prompt engineering is better

**The company's new rule:** "Prove that few-shot doesn't work before investing in fine-tuning."

**Your takeaway:** The few-shot prompting you're learning in this chapter isn't a toy technique‚Äîit's a production strategy that can save your company $150K+/year. Before you spend months fine-tuning, spend 10 minutes writing good examples.

**Cost comparison:**

- Fine-tuning: $100K first year + $50K/year ongoing
- Few-shot: $0 setup + $0 ongoing
- **Savings: $150K over 3 years**

**Accuracy difference:** 1% (92% vs 91%)

**Is 1% accuracy worth $150K?** Usually not.

---

### üêõ Error Prediction Exercise #2: Few-Shot Debugging

**Before running code, predict what happens in each scenario:**

**Scenario 1: Examples Contradict Instructions**
```python
prompt = """
Extract only the dollar amount from invoices.

Examples:
Input: "Invoice #123: $500" -> Output: Invoice #123 costs $500
Input: "Total: $1,200" -> Output: The total is $1,200

Input: "Payment due: $750" -> Output:
"""
```

**What will happen?**
- A) LLM follows instructions, returns "$750"
- B) LLM follows examples, returns "Payment due is $750"
- C) LLM gets confused, returns unpredictable output
- D) Raises validation error

<details>
<summary>üí° <strong>Answer & Explanation</strong></summary>

**Correct Answer: B (most likely)**

**Why:** LLMs learn more from examples than instructions. Examples are concrete, instructions are abstract.

**The conflict:**
- **Instruction says:** "Extract only the dollar amount" (implies "$750")
- **Examples show:** Full sentences with context (implies "Payment due is $750")

**LLM reasoning:** "The examples show full sentences, so I should output a full sentence."

**The fix: Make examples match instructions**
```python
prompt = """
Extract only the dollar amount from invoices.

Examples:
Input: "Invoice #123: $500" -> Output: $500
Input: "Total: $1,200" -> Output: $1,200

Input: "Payment due: $750" -> Output:
"""
```

**Production lesson:** Examples trump instructions. If they conflict, examples win. Make them consistent.

**Real-world impact:** A company spent 2 weeks debugging "inconsistent AI output" before realizing their examples contradicted their instructions. Cost: $20K in engineering time.

</details>

---

**Scenario 2: Too Many Examples (Context Overflow)**
```python
prompt = """
Classify sentiment as Positive, Negative, or Neutral.

[... 50 examples ...]

Input: "This product is okay." -> Output:
"""
```

**What will happen?**
- A) Perfect accuracy (more examples = better)
- B) LLM ignores most examples, uses only first/last few
- C) Context limit exceeded, request fails
- D) LLM gets confused, accuracy decreases

<details>
<summary>üí° <strong>Answer & Explanation</strong></summary>

**Correct Answer: B or C (depends on model)**

**Why:** LLMs have context limits (4K-128K tokens). Too many examples cause problems.

**The "Lost in the Middle" phenomenon:**
- LLMs pay most attention to the **beginning** and **end** of prompts
- Middle examples get "lost" and ignored
- More examples ‚â† better performance

**Optimal number of examples:**
- **3-5 examples:** Sweet spot for most tasks
- **1-2 examples:** Simple tasks (sentiment, classification)
- **5-10 examples:** Complex tasks (code generation, legal analysis)
- **10+ examples:** Rarely helpful, often harmful

**The fix: Use fewer, better examples**
```python
prompt = """
Classify sentiment as Positive, Negative, or Neutral.

Examples:
Input: "This is amazing!" -> Output: Positive
Input: "This is terrible." -> Output: Negative
Input: "This is okay." -> Output: Neutral

Input: "This product is okay." -> Output:
"""
```

**Production lesson:** Quality > Quantity. 3 perfect examples beat 50 mediocre ones.

*tune only when stable (expensive but justified)

</details>

**Action:** For your current project, list 3 tasks. Decide: few-shot or fine-tuning? Calculate the ROI.

---
VPs. Fine-tuning is an optimization for high-volume, stable tasks.

**Production wisdom:**
> "Few-shot is your prototype. Fine-tuning is your optimization. Don't optimize prematurely." ‚Äî ML Engineer

**The "Premature Fine-Tuning" anti-pattern:**

Many companies waste money fine-tuning before validating:
1. Spend $50K fine-tuning
2. Realize task definition was wrong
3. Have to start over
4. **Total waste:** $50K

**The right approach:**
1. Validate with few-shot ($0)
2. Iterate on task definition (cheap)
3. Fine-racy: 87% (some misclassifications)

**Phase 3: Fine-tuning (Month 7)**
- Collected 10,000 labeled tickets
- Fine-tuned GPT-3.5
- Accuracy: 94%
- Cost: $20K setup, $500/month ongoing
- **ROI:** Saved $2,000/month, improved customer satisfaction

**The calculation:**
- Few-shot: $2,500/month, 87% accuracy
- Fine-tuning: $500/month + $20K/12 = $2,167/month, 94% accuracy
- **Break-even:** 12 months
- **After 12 months:** Saving $2,000/month + better accuracy

**Key lesson:** Few-shot is perfect for validation and Muracy** (is 90% good enough?)
3. **Calculate ROI** (is 5% more accuracy worth $50K?)
4. **Fine-tune if justified** (high volume + high accuracy requirement)

**Real-world example:**

**Company:** Customer support automation
**Task:** Classify support tickets (10 categories)

**Phase 1: Few-shot (Week 1)**
- 3 examples per category
- Accuracy: 87%
- Cost: $0 setup, $500/month
- **Decision:** Good enough for MVP

**Phase 2: Production (Months 1-6)**
- Volume: 50,000 tickets/month
- Few-shot cost: $2,500/month
- Accu- ‚úÖ Simple to moderate tasks
- ‚úÖ Low to medium volume
- ‚úÖ Requirements change frequently
- ‚úÖ Limited budget
- ‚úÖ Need to deploy quickly
- ‚úÖ 85-95% accuracy is acceptable

**When to use Fine-Tuning:**
- ‚úÖ Complex, specialized tasks
- ‚úÖ High volume (cost-effective at scale)
- ‚úÖ Stable requirements
- ‚úÖ Large budget
- ‚úÖ Need 95%+ accuracy
- ‚úÖ Latency is critical
- ‚úÖ Have large labeled dataset

**The Hybrid Approach:**

Many companies use both:
1. **Start with few-shot** (validate the task is solvable)
2. **Measure acc*The Decision Matrix:**

| Factor | Few-Shot | Fine-Tuning |
|--------|----------|-------------|
| **Setup cost** | $0 | $10K-$50K |
| **Setup time** | Minutes | Weeks/months |
| **Ongoing cost** | API calls only | API + maintenance |
| **Accuracy** | 85-95% | 90-99% |
| **Latency** | Normal | Faster |
| **Flexibility** | Easy to change | Hard to change |
| **Data required** | 3-10 examples | 1,000-10,000 examples |
| **Expertise required** | Prompt engineering | ML engineering |

**When to use Few-Shot:**
** Complex task (20+ fields), high accuracy requirement
- **Approach:** Start with 5-10 examples per field, fine-tune if accuracy < 95%
- **Cost:** Few-shot: $0 setup, $200/month | Fine-tuning: $30K setup, $1K/month

**Scenario C: Custom Code Generation**
- ‚úÖ **Use few-shot**
- **Why:** Low volume, examples easy to create, requirements change frequently
- **Cost:** $0 setup, $100/month in API calls
- **Alternative:** Fine-tuning would cost $20K setup, but requirements change weekly (expensive to retrain)

*sider:**
- Which scenarios need fine-tuning?
- Which can use few-shot?
- What are the trade-offs?

<details>
<summary>üí° <strong>Reflection Guide</strong></summary>

**Scenario A: Sentiment Analysis**
- ‚úÖ **Use few-shot**
- **Why:** Simple task, 3 examples enough, high volume makes fine-tuning expensive
- **Cost:** $0 setup, ~$500/month in API calls
- **Alternative:** Fine-tuning would cost $10K setup + $2K/month

**Scenario B: Legal Contract Extraction**
- ‚ö†Ô∏è **Try few-shot first, fine-tune if needed**
- **Why:os:**

**Scenario A: Sentiment analysis (Positive/Negative/Neutral)**
- Task: Classify customer reviews
- Volume: 10,000 reviews/day
- Accuracy requirement: 85%+
- Budget: $5,000

**Scenario B: Legal contract extraction**
- Task: Extract 20+ fields from contracts
- Volume: 100 contracts/day
- Accuracy requirement: 95%+
- Budget: $50,000

**Scenario C: Custom code generation**
- Task: Generate React components from descriptions
- Volume: 50 requests/day
- Accuracy requirement: 90%+
- Budget: $10,000

**Questions to conrmat from examples, neither can the LLM.

</details>

---

**Reflection:** Which error would be hardest to debug in production? Why?

**Key takeaway:** Few-shot prompting is powerful but fragile. Small inconsistencies in examples cause big problems in output. Spend time crafting perfect examples‚Äîit's worth it.

---

### üß† Metacognitive Checkpoint #3: Few-Shot vs Fine-Tuning

**Pause and reflect:**

You've learned few-shot prompting. But when should you use it vs fine-tuning a model?

**Consider these scenariat ("37C")

**LLM reasoning:** "Sometimes short, sometimes long. I'll guess."

**The fix: Consistent format across all examples**
```python
prompt = """
Convert temperatures from Fahrenheit to Celsius. Return only the number with 'C'.

Examples:
Input: "32F" -> Output: 0C
Input: "212F" -> Output: 100C
Input: "98.6F" -> Output: 37C

Input: "72F" -> Output:
"""
```

**Production lesson:** Consistency is key. All examples should follow the exact same format.

**Testing tip:** If you can't predict the output fod example format)
- B) LLM returns "Room temperature is 22.2 degrees Celsius" (follows second example)
- C) LLM returns unpredictable format
- D) Raises validation error

<details>
<summary>üí° <strong>Answer & Explanation</strong></summary>

**Correct Answer: C (unpredictable)**

**Why:** Inconsistent examples confuse the LLM. It doesn't know which format to follow.

**The inconsistency:**
- Example 1: Short format ("0C")
- Example 2: Long format ("Water boils at 100 degrees Celsius")
- Example 3: Short form phone number examples "to make it more robust." It made it less robust. Cost: 500 customer complaints.

</details>

---

**Scenario 4: Inconsistent Example Format**
```python
prompt = """
Convert temperatures from Fahrenheit to Celsius.

Examples:
Input: "32F" -> Output: 0C
Input: "Water boils at 212 degrees Fahrenheit" -> Output: Water boils at 100 degrees Celsius
Input: "98.6F" -> Output: 37C

Input: "Room temperature is 72F" -> Output:
"""
```

**What will happen?**
- A) LLM returns "22.2C" (follows first/thir: Make examples match the task exactly**
```python
prompt = """
Extract email addresses from text.

Examples:
Input: "Contact us at info@example.com" -> Output: info@example.com
Input: "Email: support@company.org" -> Output: support@company.org

Input: "Contact us at support@example.com" -> Output:
"""
```

**Production lesson:** Examples define the task more than instructions do. Make them match exactly.

**Real-world bug:** A company's "extract email" feature started returning phone numbers after someone addedres irrelevant examples)
- B) LLM extracts phone number or URL (follows example pattern)
- C) LLM returns empty output
- D) Raises validation error

<details>
<summary>üí° <strong>Answer & Explanation</strong></summary>

**Correct Answer: B (most likely)**

**Why:** LLM learns the pattern "extract contact info" from examples, not "extract emails specifically."

**The mismatch:**
- **Task:** Extract emails
- **Examples:** Extract phone numbers and URLs
- **LLM learns:** "Extract any contact-like string"

**The fix*Cost impact:**
- 50 examples: ~2,000 tokens = $0.02/request (GPT-4)
- 3 examples: ~200 tokens = $0.002/request
- **Savings: 10x cheaper, same accuracy**

</details>

---

**Scenario 3: Examples Don't Match Task**
```python
prompt = """
Extract email addresses from text.

Examples:
Input: "Call me at 555-1234" -> Output: 555-1234
Input: "Visit www.example.com" -> Output: www.example.com

Input: "Contact us at support@example.com" -> Output:
"""
```

**What will happen?**
- A) LLM extracts email correctly (igno## Common Mistakes

### Mistake #1: Loose Braces `{}` in Prompts

If you are generating JSON examples inside a Python f-string or `.format()` call, you need to escape braces.
**Bad**: `template = "Return JSON: {"key": "value"}"` (Python thinks `{key}` is a variable).
**Good**: `template = "Return JSON: {{ \"key\": \"value\" }}"` (Double braces escape them).

### Mistake #2: Huge Prompts

Don't put 50 examples in the prompt. It costs money (tokens) and confuses the model ("Lost in the Middle" phenomenon). 3-5 examples is usually the sweet spot.

### Mistake #3: Silent Failures

If you use simple string replacement (`str.replace()`) instead of `.format()`, you won't get errors if you miss a variable. The prompt will just contain `{name}` literally, and the LLM will be very confused.

---

## Quick Reference Card

### CIF Pattern

- **C**ontext (Persona)
- **I**nstructions (Task)
- **F**ormat (Output style)

### Python Formatting

```python
template = "Hello {name}"
output = template.format(name="Ahmed")
```

### Escaping JSON in Templates

```python
# To output: {"status": "ok"}
template = "{{ \"status\": \"ok\" }}"
```

---

## üéØ Confidence Calibration Check

**Before moving to verification, assess your understanding:**

Rate your confidence (1-5) on each skill:

**1. CIF Pattern Application**

- Can you identify Context, Instructions, and Format in a prompt?
- Can you write a prompt using the CIF pattern?
- **Self-rating:** \_\_\_/5

**2. Template Variable Substitution**

- Can you create a `PromptTemplate` with variables?
- Can you predict errors from missing/extra variables?
- **Self-rating:** \_\_\_/5

**3. Few-Shot Example Design**

- Can you write 3-5 effective examples for a task?
- Can you identify when examples contradict instructions?
- **Self-rating:** \_\_\_/5

**4. Prompt Debugging**

- Can you debug template validation errors?
- Can you fix inconsistent few-shot examples?
- **Self-rating:** \_\_\_/5

**5. Production Prompt Management**

- Do you understand when to use templates vs f-strings?
- Can you design a centralized prompt registry?
- **Self-rating:** \_\_\_/5

<details>
<summary>üí° <strong>Calibration Guide</strong></summary>

**If you rated yourself 4-5 on all skills:**

- ‚úÖ You're ready for verification
- ‚úÖ Proceed with confidence
- ‚úÖ Consider helping others learn

**If you rated yourself 3 on any skill:**

- ‚ö†Ô∏è Review that section before verification
- ‚ö†Ô∏è Try the hands-on exercises again
- ‚ö†Ô∏è You'll probably pass verification but might struggle

**If you rated yourself 1-2 on any skill:**

- ‚ùå Re-read that section carefully
- ‚ùå Complete all hands-on exercises
- ‚ùå Don't proceed to verification yet

**Calibration check:**

- Did you actually complete all 3 "Try This!" exercises?
- Did you attempt the error prediction exercises?
- Did you read the war stories and reflect on the lessons?

**If you skipped exercises:** Your confidence ratings are probably inflated. Go back and do the hands-on work.

**Production wisdom:**

> "Confidence without competence is dangerous. Competence without confidence is wasted. Calibrate both." ‚Äî Engineering Manager

</details>

**Action:** If any rating is below 3, review that section now before proceeding to verification.

---

## Verification (REQUIRED SECTION)

Let's verify your Prompting System.

**Create `verify_prompts.py`**:

```python
"""
Verification script for Chapter 9.
"""
from shared.utils.prompting import PromptTemplate
import sys

print("üß™ Running Prompt Engineering Verification...\n")

# Test 1: Basic Substitution
print("Test 1: Variable Substitution...")
try:
    pt = PromptTemplate("Hello {name}", ["name"])
    res = pt.format(name="World")
    assert res == "Hello World"
    print("‚úÖ Substitution works")
except Exception as e:
    print(f"‚ùå Failed: {e}")
    sys.exit(1)

# Test 2: Validation (Missing Variable)
print("Test 2: Validation Logic...")
try:
    pt.format()  # Missing 'name'
    print("‚ùå Failed: Should have raised error")
    sys.exit(1)
except ValueError as e:
    assert "Missing" in str(e)
    print("‚úÖ Caught missing variable error")

# Test 3: Validation (Template Mismatch)
print("Test 3: Template Definition Validation...")
try:
    # 'age' is in input_variables but not in string
    PromptTemplate("Hello {name}", ["name", "age"])
    print("‚ùå Failed: Should have detected missing placeholder")
    sys.exit(1)
except ValueError as e:
    assert "expected but not found" in str(e)
    print("‚úÖ Caught definition mismatch")

# Test 4: Multiple Variables
print("Test 4: Multiple Variable Substitution...")
try:
    pt = PromptTemplate(
        "Hello {name}, you are {age} years old",
        ["name", "age"]
    )
    res = pt.format(name="Alice", age=30)
    assert "Alice" in res and "30" in res
    print("‚úÖ Multiple variables work")
except Exception as e:
    print(f"‚ùå Failed: {e}")
    sys.exit(1)

# Test 5: JSON Escaping
print("Test 5: JSON Brace Escaping...")
try:
    pt = PromptTemplate(
        'Return JSON: {{"status": "{status}"}}',
        ["status"]
    )
    res = pt.format(status="ok")
    assert '{"status": "ok"}' in res
    print("‚úÖ JSON escaping works")
except Exception as e:
    print(f"‚ùå Failed: {e}")
    sys.exit(1)

print("\nüéâ Chapter 9 Complete! You can now manage prompts like code.")
```

**Run it:** `python verify_prompts.py`

### Expected output:

```
üß™ Running Prompt Engineering Verification...

Test 1: Variable Substitution...
‚úÖ Substitution works
Test 2: Validation Logic...
‚úÖ Caught missing variable error
Test 3: Template Definition Validation...
‚úÖ Caught definition mismatch
Test 4: Multiple Variable Substitution...
‚úÖ Multiple variables work
Test 5: JSON Brace Escaping...
‚úÖ JSON escaping works

üéâ Chapter 9 Complete! You can now manage prompts like code.
```

**If any test fails:** Review the corresponding section and fix your implementation before continuing.

---

## üó∫Ô∏è Concept Map: Prompt Engineering Ecosystem

```
                    PROMPT ENGINEERING
                           |
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        |                  |                  |
    STRUCTURE          TEMPLATES         FEW-SHOT
        |                  |                  |
   CIF Pattern      PromptTemplate      Examples
        |                  |                  |
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  |     |     |      |    |    |       |    |    |
Context Inst Format Vars Valid Reuse  Zero 3-5  Fine-tune
                                       Shot Exs  (expensive)

BACKWARD CONNECTIONS:
‚Üê Chapter 7: Message roles (system, user, assistant)
‚Üê Chapter 8: MultiProviderClient (test prompts across providers)

FORWARD CONNECTIONS:
‚Üí Chapter 10: Streaming (real-time prompt responses)
‚Üí Chapter 17: RAG (prompt templates for retrieval)
‚Üí Chapter 26: Agents (advanced prompt engineering)

CROSS-CUTTING CONCERNS:
‚Ä¢ Token economics (longer prompts = higher cost)
‚Ä¢ Version control (prompts as code)
‚Ä¢ Testing (validate prompts like functions)
‚Ä¢ Production patterns (centralized registries)
```

**Key relationships:**

- **CIF + Templates** = Reusable, validated prompts
- **Templates + Few-Shot** = Production-ready prompt system
- **Few-Shot vs Fine-Tuning** = Cost/accuracy trade-off

**Mental model:** Prompts are specifications. Treat them with the same rigor as database schemas or API contracts.

---

## Summary

**What you learned:**

1. ‚úÖ **Prompt anatomy** ‚Äî CIF pattern (Context, Instructions, Format)
2. ‚úÖ **Template systems** ‚Äî Reusable prompts with variable substitution and validation
3. ‚úÖ **Few-shot learning** ‚Äî Teaching through 3-5 examples for consistent output
4. ‚úÖ **Production patterns** ‚Äî Centralized prompt management, version control
5. ‚úÖ **Cost optimization** ‚Äî Few-shot vs fine-tuning trade-offs
6. ‚úÖ **Error prevention** ‚Äî Template validation catches bugs before production
7. ‚úÖ **Real-world lessons** ‚Äî $80K prompt versioning disaster, $150K saved by few-shot

**Key Takeaway**: A prompt is not just a question. It is a precise specification. Treat it with the same care you treat your database schema.

**Skills unlocked**: üéØ

- Prompt Engineering (CIF pattern)
- String Interpolation & Templating
- Few-Shot Prompting
- Production Prompt Management

**Production wisdom you gained:**

- "Prompts are configuration, not code" ‚Äî Centralize them
- "Examples trump instructions" ‚Äî Make them consistent
- "Try few-shot before fine-tuning" ‚Äî Save $150K/year
- "Specificity is a dial, not a switch" ‚Äî Find your Goldilocks Zone

**Looking ahead**: Now we have a Client (Ch 8) and a Prompt System (Ch 9). In **Chapter 10**, we will learn how to make the AI respond **word-by-word** (Streaming) for that real-time "thinking" feel!

---

**Next**: [Chapter 10: Streaming Responses ‚Üí](chapter-10-streaming-responses.md)

## Common Mistakes

### Mistake #1: Loose Braces `{}` in Prompts

If you are generating JSON examples inside a Python f-string or `.format()` call, you need to escape braces.
**Bad**: `template = "Return JSON: {"key": "value"}"` (Python thinks `{key}` is a variable).
**Good**: `template = "Return JSON: {{ \"key\": \"value\" }}"` (Double braces escape them).

### Mistake #2: Huge Prompts

Don't put 50 examples in the prompt. It costs money (tokens) and confuses the model ("Lost in the Middle" phenomenon). 3-5 examples is usually the sweet spot.

### Mistake #3: Silent Failures

If you use simple string replacement (`str.replace()`) instead of `.format()`, you won't get errors if you miss a variable. The prompt will just contain `{name}` literally, and the LLM will be very confused.

---

## Quick Reference Card

### CIF Pattern

- **C**ontext (Persona)
- **I**nstructions (Task)
- **F**ormat (Output style)

### Python Formatting

```python
template = "Hello {name}"
output = template.format(name="Ahmed")
```

### Escaping JSON in Templates

```python
# To output: {"status": "ok"}
template = "{{ \"status\": \"ok\" }}"
```

---

## üéØ Confidence Calibration Check

**Before moving to verification, assess your understanding:**

Rate your confidence (1-5) on each skill:

**1. CIF Pattern Application**

- Can you identify Context, Instructions, and Format in a prompt?
- Can you write a prompt using the CIF pattern?
- **Self-rating:** \_\_\_/5

**2. Template Variable Substitution**

- Can you create a `PromptTemplate` with variables?
- Can you predict errors from missing/extra variables?
- **Self-rating:** \_\_\_/5

**3. Few-Shot Example Design**

- Can you write 3-5 effective examples for a task?
- Can you identify when examples contradict instructions?
- **Self-rating:** \_\_\_/5

**4. Prompt Debugging**

- Can you debug template validation errors?
- Can you fix inconsistent few-shot examples?
- **Self-rating:** \_\_\_/5

**5. Production Prompt Management**

- Do you understand when to use templates vs f-strings?
- Can you design a centralized prompt registry?
- **Self-rating:** \_\_\_/5

<details>
<summary>üí° <strong>Calibration Guide</strong></summary>

**If you rated yourself 4-5 on all skills:**

- ‚úÖ You're ready for verification
- ‚úÖ Proceed with confidence
- ‚úÖ Consider helping others learn

**If you rated yourself 3 on any skill:**

- ‚ö†Ô∏è Review that section before verification
- ‚ö†Ô∏è Try the hands-on exercises again
- ‚ö†Ô∏è You'll probably pass verification but might struggle

**If you rated yourself 1-2 on any skill:**

- ‚ùå Re-read that section carefully
- ‚ùå Complete all hands-on exercises
- ‚ùå Don't proceed to verification yet

**Calibration check:**

- Did you actually complete all 3 "Try This!" exercises?
- Did you attempt the error prediction exercises?
- Did you read the war stories and reflect on the lessons?

**If you skipped exercises:** Your confidence ratings are probably inflated. Go back and do the hands-on work.

**Production wisdom:**

> "Confidence without competence is dangerous. Competence without confidence is wasted. Calibrate both." ‚Äî Engineering Manager

</details>

**Action:** If any rating is below 3, review that section now before proceeding to verification.

---

## Verification (REQUIRED SECTION)

Let's verify your Prompting System.

**Create `verify_prompts.py`**:

```python
"""
Verification script for Chapter 9.
"""
from shared.utils.prompting import PromptTemplate
import sys

print("üß™ Running Prompt Engineering Verification...\n")

# Test 1: Basic Substitution
print("Test 1: Variable Substitution...")
try:
    pt = PromptTemplate("Hello {name}", ["name"])
    res = pt.format(name="World")
    assert res == "Hello World"
    print("‚úÖ Substitution works")
except Exception as e:
    print(f"‚ùå Failed: {e}")
    sys.exit(1)

# Test 2: Validation (Missing Variable)
print("Test 2: Validation Logic...")
try:
    pt.format()  # Missing 'name'
    print("‚ùå Failed: Should have raised error")
    sys.exit(1)
except ValueError as e:
    assert "Missing" in str(e)
    print("‚úÖ Caught missing variable error")

# Test 3: Validation (Template Mismatch)
print("Test 3: Template Definition Validation...")
try:
    # 'age' is in input_variables but not in string
    PromptTemplate("Hello {name}", ["name", "age"])
    print("‚ùå Failed: Should have detected missing placeholder")
    sys.exit(1)
except ValueError as e:
    assert "expected but not found" in str(e)
    print("‚úÖ Caught definition mismatch")

# Test 4: Multiple Variables
print("Test 4: Multiple Variable Substitution...")
try:
    pt = PromptTemplate(
        "Hello {name}, you are {age} years old",
        ["name", "age"]
    )
    res = pt.format(name="Alice", age=30)
    assert "Alice" in res and "30" in res
    print("‚úÖ Multiple variables work")
except Exception as e:
    print(f"‚ùå Failed: {e}")
    sys.exit(1)

# Test 5: JSON Escaping
print("Test 5: JSON Brace Escaping...")
try:
    pt = PromptTemplate(
        'Return JSON: {{"status": "{status}"}}',
        ["status"]
    )
    res = pt.format(status="ok")
    assert '{"status": "ok"}' in res
    print("‚úÖ JSON escaping works")
except Exception as e:
    print(f"‚ùå Failed: {e}")
    sys.exit(1)

print("\nüéâ Chapter 9 Complete! You can now manage prompts like code.")
```

**Run it:** `python verify_prompts.py`

### Expected output:

```
üß™ Running Prompt Engineering Verification...

Test 1: Variable Substitution...
‚úÖ Substitution works
Test 2: Validation Logic...
‚úÖ Caught missing variable error
Test 3: Template Definition Validation...
‚úÖ Caught definition mismatch
Test 4: Multiple Variable Substitution...
‚úÖ Multiple variables work
Test 5: JSON Brace Escaping...
‚úÖ JSON escaping works

üéâ Chapter 9 Complete! You can now manage prompts like code.
```

**If any test fails:** Review the corresponding section and fix your implementation before continuing.

---

## üó∫Ô∏è Concept Map: Prompt Engineering Ecosystem

```
                    PROMPT ENGINEERING
                           |
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        |                  |                  |
    STRUCTURE          TEMPLATES         FEW-SHOT
        |                  |                  |
   CIF Pattern      PromptTemplate      Examples
        |                  |                  |
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  |     |     |      |    |    |       |    |    |
Context Inst Format Vars Valid Reuse  Zero 3-5  Fine-tune
                                       Shot Exs  (expensive)

BACKWARD CONNECTIONS:
‚Üê Chapter 7: Message roles (system, user, assistant)
‚Üê Chapter 8: MultiProviderClient (test prompts across providers)

FORWARD CONNECTIONS:
‚Üí Chapter 10: Streaming (real-time prompt responses)
‚Üí Chapter 17: RAG (prompt templates for retrieval)
‚Üí Chapter 26: Agents (advanced prompt engineering)

CROSS-CUTTING CONCERNS:
‚Ä¢ Token economics (longer prompts = higher cost)
‚Ä¢ Version control (prompts as code)
‚Ä¢ Testing (validate prompts like functions)
‚Ä¢ Production patterns (centralized registries)
```

**Key relationships:**

- **CIF + Templates** = Reusable, validated prompts
- **Templates + Few-Shot** = Production-ready prompt system
- **Few-Shot vs Fine-Tuning** = Cost/accuracy trade-off

**Mental model:** Prompts are specifications. Treat them with the same rigor as database schemas or API contracts.

---

## Summary

**What you learned:**

1. ‚úÖ **Prompt anatomy** ‚Äî CIF pattern (Context, Instructions, Format)
2. ‚úÖ **Template systems** ‚Äî Reusable prompts with variable substitution and validation
3. ‚úÖ **Few-shot learning** ‚Äî Teaching through 3-5 examples for consistent output
4. ‚úÖ **Production patterns** ‚Äî Centralized prompt management, version control
5. ‚úÖ **Cost optimization** ‚Äî Few-shot vs fine-tuning trade-offs
6. ‚úÖ **Error prevention** ‚Äî Template validation catches bugs before production
7. ‚úÖ **Real-world lessons** ‚Äî $80K prompt versioning disaster, $150K saved by few-shot

**Key Takeaway**: A prompt is not just a question. It is a precise specification. Treat it with the same care you treat your database schema.

**Skills unlocked**: üéØ

- Prompt Engineering (CIF pattern)
- String Interpolation & Templating
- Few-Shot Prompting
- Production Prompt Management

**Production wisdom you gained:**

- "Prompts are configuration, not code" ‚Äî Centralize them
- "Examples trump instructions" ‚Äî Make them consistent
- "Try few-shot before fine-tuning" ‚Äî Save $150K/year
- "Specificity is a dial, not a switch" ‚Äî Find your Goldilocks Zone

**Looking ahead**: Now we have a Client (Ch 8) and a Prompt System (Ch 9). In **Chapter 10**, we will learn how to make the AI respond **word-by-word** (Streaming) for that real-time "thinking" feel!

---

**Next**: [Chapter 10: Streaming Responses ‚Üí](chapter-10-streaming-responses.md)

## Common Mistakes

### Mistake #1: Loose Braces `{}` in Prompts

If you are generating JSON examples inside a Python f-string or `.format()` call, you need to escape braces.
**Bad**: `template = "Return JSON: {"key": "value"}"` (Python thinks `{key}` is a variable).
**Good**: `template = "Return JSON: {{ \"key\": \"value\" }}"` (Double braces escape them).

### Mistake #2: Huge Prompts

Don't put 50 examples in the prompt. It costs money (tokens) and confuses the model ("Lost in the Middle" phenomenon). 3-5 examples is usually the sweet spot.

### Mistake #3: Silent Failures

If you use simple string replacement (`str.replace()`) instead of `.format()`, you won't get errors if you miss a variable. The prompt will just contain `{name}` literally, and the LLM will be very confused.

---

## Quick Reference Card

### CIF Pattern

- **C**ontext (Persona)
- **I**nstructions (Task)
- **F**ormat (Output style)

### Python Formatting

```python
template = "Hello {name}"
output = template.format(name="Ahmed")
```

### Escaping JSON in Templates

```python
# To output: {"status": "ok"}
template = "{{ \"status\": \"ok\" }}"
```

---

## üéØ Confidence Calibration Check

**Before moving to verification, assess your understanding:**

Rate your confidence (1-5) on each skill:

**1. CIF Pattern Application**

- Can you identify Context, Instructions, and Format in a prompt?
- Can you write a prompt using the CIF pattern?
- **Self-rating:** \_\_\_/5

**2. Template Variable Substitution**

- Can you create a `PromptTemplate` with variables?
- Can you predict errors from missing/extra variables?
- **Self-rating:** \_\_\_/5

**3. Few-Shot Example Design**

- Can you write 3-5 effective examples for a task?
- Can you identify when examples contradict instructions?
- **Self-rating:** \_\_\_/5

**4. Prompt Debugging**

- Can you debug template validation errors?
- Can you fix inconsistent few-shot examples?
- **Self-rating:** \_\_\_/5

**5. Production Prompt Management**

- Do you understand when to use templates vs f-strings?
- Can you design a centralized prompt registry?
- **Self-rating:** \_\_\_/5

<details>
<summary>üí° <strong>Calibration Guide</strong></summary>

**If you rated yourself 4-5 on all skills:**

- ‚úÖ You're ready for verification
- ‚úÖ Proceed with confidence
- ‚úÖ Consider helping others learn

**If you rated yourself 3 on any skill:**

- ‚ö†Ô∏è Review that section before verification
- ‚ö†Ô∏è Try the hands-on exercises again
- ‚ö†Ô∏è You'll probably pass verification but might struggle

**If you rated yourself 1-2 on any skill:**

- ‚ùå Re-read that section carefully
- ‚ùå Complete all hands-on exercises
- ‚ùå Don't proceed to verification yet

**Calibration check:**

- Did you actually complete all 3 "Try This!" exercises?
- Did you attempt the error prediction exercises?
- Did you read the war stories and reflect on the lessons?

**If you skipped exercises:** Your confidence ratings are probably inflated. Go back and do the hands-on work.

**Production wisdom:**

> "Confidence without competence is dangerous. Competence without confidence is wasted. Calibrate both." ‚Äî Engineering Manager

</details>

**Action:** If any rating is below 3, review that section now before proceeding to verification.

---

## Verification (REQUIRED SECTION)

Let's verify your Prompting System.

**Create `verify_prompts.py`**:

```python
"""
Verification script for Chapter 9.
"""
from shared.utils.prompting import PromptTemplate
import sys

print("üß™ Running Prompt Engineering Verification...\n")

# Test 1: Basic Substitution
print("Test 1: Variable Substitution...")
try:
    pt = PromptTemplate("Hello {name}", ["name"])
    res = pt.format(name="World")
    assert res == "Hello World"
    print("‚úÖ Substitution works")
except Exception as e:
    print(f"‚ùå Failed: {e}")
    sys.exit(1)

# Test 2: Validation (Missing Variable)
print("Test 2: Validation Logic...")
try:
    pt.format()  # Missing 'name'
    print("‚ùå Failed: Should have raised error")
    sys.exit(1)
except ValueError as e:
    assert "Missing" in str(e)
    print("‚úÖ Caught missing variable error")

# Test 3: Validation (Template Mismatch)
print("Test 3: Template Definition Validation...")
try:
    # 'age' is in input_variables but not in string
    PromptTemplate("Hello {name}", ["name", "age"])
    print("‚ùå Failed: Should have detected missing placeholder")
    sys.exit(1)
except ValueError as e:
    assert "expected but not found" in str(e)
    print("‚úÖ Caught definition mismatch")

# Test 4: Multiple Variables
print("Test 4: Multiple Variable Substitution...")
try:
    pt = PromptTemplate(
        "Hello {name}, you are {age} years old",
        ["name", "age"]
    )
    res = pt.format(name="Alice", age=30)
    assert "Alice" in res and "30" in res
    print("‚úÖ Multiple variables work")
except Exception as e:
    print(f"‚ùå Failed: {e}")
    sys.exit(1)

# Test 5: JSON Escaping
print("Test 5: JSON Brace Escaping...")
try:
    pt = PromptTemplate(
        'Return JSON: {{"status": "{status}"}}',
        ["status"]
    )
    res = pt.format(status="ok")
    assert '{"status": "ok"}' in res
    print("‚úÖ JSON escaping works")
except Exception as e:
    print(f"‚ùå Failed: {e}")
    sys.exit(1)

print("\nüéâ Chapter 9 Complete! You can now manage prompts like code.")
```

**Run it:** `python verify_prompts.py`

### Expected output:

```
üß™ Running Prompt Engineering Verification...

Test 1: Variable Substitution...
‚úÖ Substitution works
Test 2: Validation Logic...
‚úÖ Caught missing variable error
Test 3: Template Definition Validation...
‚úÖ Caught definition mismatch
Test 4: Multiple Variable Substitution...
‚úÖ Multiple variables work
Test 5: JSON Brace Escaping...
‚úÖ JSON escaping works

üéâ Chapter 9 Complete! You can now manage prompts like code.
```

**If any test fails:** Review the corresponding section and fix your implementation before continuing.

---

## üó∫Ô∏è Concept Map: Prompt Engineering Ecosystem

```
                    PROMPT ENGINEERING
                           |
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        |                  |                  |
    STRUCTURE          TEMPLATES         FEW-SHOT
        |                  |                  |
   CIF Pattern      PromptTemplate      Examples
        |                  |                  |
  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
  |     |     |      |    |    |       |    |    |
Context Inst Format Vars Valid Reuse  Zero 3-5  Fine-tune
                                       Shot Exs  (expensive)

BACKWARD CONNECTIONS:
‚Üê Chapter 7: Message roles (system, user, assistant)
‚Üê Chapter 8: MultiProviderClient (test prompts across providers)

FORWARD CONNECTIONS:
‚Üí Chapter 10: Streaming (real-time prompt responses)
‚Üí Chapter 17: RAG (prompt templates for retrieval)
‚Üí Chapter 26: Agents (advanced prompt engineering)

CROSS-CUTTING CONCERNS:
‚Ä¢ Token economics (longer prompts = higher cost)
‚Ä¢ Version control (prompts as code)
‚Ä¢ Testing (validate prompts like functions)
‚Ä¢ Production patterns (centralized registries)
```

**Key relationships:**

- **CIF + Templates** = Reusable, validated prompts
- **Templates + Few-Shot** = Production-ready prompt system
- **Few-Shot vs Fine-Tuning** = Cost/accuracy trade-off

**Mental model:** Prompts are specifications. Treat them with the same rigor as database schemas or API contracts.

---

## Summary

**What you learned:**

1. ‚úÖ **Prompt anatomy** ‚Äî CIF pattern (Context, Instructions, Format)
2. ‚úÖ **Template systems** ‚Äî Reusable prompts with variable substitution and validation
3. ‚úÖ **Few-shot learning** ‚Äî Teaching through 3-5 examples for consistent output
4. ‚úÖ **Production patterns** ‚Äî Centralized prompt management, version control
5. ‚úÖ **Cost optimization** ‚Äî Few-shot vs fine-tuning trade-offs
6. ‚úÖ **Error prevention** ‚Äî Template validation catches bugs before production
7. ‚úÖ **Real-world lessons** ‚Äî $80K prompt versioning disaster, $150K saved by few-shot

**Key Takeaway**: A prompt is not just a question. It is a precise specification. Treat it with the same care you treat your database schema.

**Skills unlocked**: üéØ

- Prompt Engineering (CIF pattern)
- String Interpolation & Templating
- Few-Shot Prompting
- Production Prompt Management

**Production wisdom you gained:**

- "Prompts are configuration, not code" ‚Äî Centralize them
- "Examples trump instructions" ‚Äî Make them consistent
- "Try few-shot before fine-tuning" ‚Äî Save $150K/year
- "Specificity is a dial, not a switch" ‚Äî Find your Goldilocks Zone

**Looking ahead**: Now we have a Client (Ch 8) and a Prompt System (Ch 9). In **Chapter 10**, we will learn how to make the AI respond **word-by-word** (Streaming) for that real-time "thinking" feel!

---

**Next**: [Chapter 10: Streaming Responses ‚Üí](chapter-10-streaming-responses.md)
